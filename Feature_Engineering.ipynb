{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q1- What is a parameter?**\n",
        "\n",
        "ANS- A parameter is an internal variable or coefficient in a model that the algorithm learns from the training data.In machine learning, parameters are the internal values that the algorithm learns from the training data. These values are adjusted during the training process to minimize the error or loss function, helping the model make accurate predictions.\n",
        "\n",
        "**Example:** In linear regression\n",
        "\n",
        "y=mx+c, m and c are parameters"
      ],
      "metadata": {
        "id": "CtB2_zcNg0uE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2- What is correlation?\n",
        "What does negative correlation mean?**\n",
        "\n",
        "ANS- Correlation is a statistical measure that describes the strength and direction of a relationship between two variables. It indicates how one variable changes in relation to another.The correlation value ranges from –1 to +1, where +1 represents a perfect positive relationship (both variables increase together).\n",
        "\n",
        "A negative correlation, on the other hand, means that as one variable increases, the other variable decreases. It reflects an inverse relationship between the two. For instance, there is a negative correlation between the price of a product and its demand—when the price rises, demand generally falls, and vice versa."
      ],
      "metadata": {
        "id": "cSLU1P_zg0rQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3- Define Machine Learning. What are the main components in Machine Learning?**\n",
        "\n",
        "ANS- Machine Learning (ML) is a branch of Artificial Intelligence (AI) that enables computers to learn patterns and make decisions from data without being explicitly programmed. Instead of relying on fixed rules, machine learning algorithms use data to identify relationships, predict outcomes, or make classifications.\n",
        "\n",
        "The main components of Machine Learning are:\n",
        "\n",
        "**Data**– The raw information used for training and testing the model. Example: past emails labeled as spam or not spam.\n",
        "\n",
        "**Model** – The mathematical structure that learns patterns from data (e.g., a decision tree, neural network, or regression model).\n",
        "\n",
        "**Loss Function** – A function that measures how well or poorly the model is performing by comparing predicted results with actual values.\n",
        "\n",
        "**Optimizer** – An algorithm that adjusts the model’s parameters to minimize the loss and improve accuracy.\n",
        "\n",
        "**Training and Testing Process** – The data is split into training and testing sets; the model learns from the training data and is evaluated on the test data to check its performance\n"
      ],
      "metadata": {
        "id": "thJT_MSNg0og"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4- How does loss value help in determining whether the model is good or not?**\n",
        "\n",
        "ANS- The loss value helps measure how well a machine learning model is performing by showing the difference between the model’s predictions and the actual values. A low loss value means the model’s predictions are close to the real outcomes, indicating that it has learned the patterns in the data well. Conversely, a high loss value means the predictions are far from the actual results, suggesting that the model is not performing effectively and needs improvement, such as through parameter tuning, more data, or a different algorithm.\n",
        "\n",
        "**For example**- in a house price prediction model, the loss function calculates how far the predicted house prices are from the actual prices. If the model predicts a house price as ₹50,00,000 but the actual price is ₹52,00,000, the loss (error) is small, which indicates a good model. However, if the model predicts ₹70,00,000, the loss is large, showing that the model’s accuracy is poor. Hence, minimizing the loss value is crucial for building a reliable and accurate model."
      ],
      "metadata": {
        "id": "2PoBbMZ_g0lw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5- What are continuous and categorical variables?**\n",
        "\n",
        "ANS- A continuous variable is a numerical variable that can take any value within a given range. These values are measurable and can have decimals or fractions. Continuous variables usually represent quantities that can vary smoothly. For example, height, weight, temperature, and age are continuous variables because they can take on many possible values such as 5.8 feet, 60.5 kg, or 25.3°C.\n",
        "\n",
        "A categorical variable, on the other hand, represents data that can be divided into distinct groups or categories. These variables describe qualities or characteristics rather than numerical quantities. Examples include gender (male, female), color (red, blue, green), or education level (high school, bachelor’s, master’s). Some categorical variables may also have an order, such as low, medium, and high, which are called ordinal variables."
      ],
      "metadata": {
        "id": "wDRvmx0ng0ib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6- How do we handle categorical variables in Machine Learning? What are the common techniques?**\n",
        "\n",
        "ANS- In Machine Learning, categorical variables need to be converted into numerical form because most algorithms can only work with numbers, not text or labels. This process is known as encoding. Handling categorical data properly is essential to ensure the model understands relationships among categories correctly.\n",
        "\n",
        "The common techniques used to handle categorical variables are:\n",
        "\n",
        "**Label Encoding** – Each category is assigned a unique integer value.\n",
        "Example: Red = 0, Blue = 1, Green = 2.\n",
        "This method is suitable when the categories have a natural order (e.g., Low, Medium, High).\n",
        "\n",
        "**One-Hot Encoding** – Creates separate binary (0 or 1) columns for each category.\n",
        "\n",
        "**Ordinal Encoding** – Assigns integer values based on a defined ranking or order.\n",
        "Example: Low = 1, Medium = 2, High = 3.\n",
        "\n",
        "**Binary Encoding (for high-cardinality features)** – Converts categories into binary code to reduce dimensionality"
      ],
      "metadata": {
        "id": "jBIKJ4hBjpHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7- What do you mean by training and testing a dataset?**\n",
        "\n",
        "ANS- Training and testing a dataset are two essential steps in building and evaluating a machine learning model.\n",
        "\n",
        "The testing dataset, on the other hand, is a separate portion of the data that is not shown to the model during training. It is used to evaluate how well the model performs on unseen data. Testing helps measure the model’s ability to generalize — meaning how accurately it can make predictions on new, real-world data.\n",
        "\n",
        "**For example**- if you are building a model to predict students’ exam scores based on study hours, you might use 80% of the data for training and 20% for testing. The model learns from the training data and then is tested on the remaining data to check its performance and accuracy."
      ],
      "metadata": {
        "id": "ekv-0XZsjpEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8- What is sklearn.preprocessing?**\n",
        "\n",
        "ANS- sklearn.preprocessing is a module in the scikit-learn library that provides various tools and functions for data preprocessing — the process of preparing raw data before feeding it into a machine learning model. Since most ML algorithms require data to be in a specific numerical and standardized format, this module helps clean, transform, and scale the data effectively.\n",
        "\n",
        "Example:\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "-4lmNpRgjpBh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9- What is a Test set?**\n",
        "\n",
        "ANS- A test set is a portion of the dataset that is used to evaluate the performance of a trained machine learning model. It consists of data that the model has never seen during training, allowing us to assess how well the model generalizes to new, unseen data. The main purpose of the test set is to check the model’s accuracy, reliability, and real-world performance.\n",
        "\n",
        "**For example**- if you are building a model to predict house prices, you might split your dataset into 80% for training and 20% for testing. The model learns patterns such as how location and size affect price from the training data, and then you test it using the test set to see how accurately it predicts house prices it hasn’t encountered before. A model that performs well on the test set is considered to have good generalization ability."
      ],
      "metadata": {
        "id": "E0EKys5gjo-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10- How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?**\n",
        "\n",
        "ANS- In Python, we commonly use the train_test_split function from sklearn.model_selection to divide the dataset into training and testing sets. This ensures that the model is trained on one portion of the data and evaluated on another.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = features, y = target variable\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "**How do you approach a Machine Learning problem?**\n",
        "\n",
        "**Understand the problem** – Define objectives, success criteria, and the type of problem (classification, regression, clustering).\n",
        "\n",
        "**Collect and explore data** – Gather the dataset and perform Exploratory Data Analysis (EDA) to understand patterns, distributions, and relationships.\n",
        "\n",
        "**Preprocess data** – Handle missing values, encode categorical variables, scale or normalize features, and perform feature engineering if needed.\n",
        "\n",
        "**Split data** – Divide the dataset into training and testing sets (and optionally a validation set).\n",
        "\n",
        "**Select a model** – Choose a suitable algorithm based on the problem type and data characteristics.\n",
        "\n",
        "**Train the model** – Fit the model to the training data using model.fit().\n",
        "\n",
        "**Evaluate the model** – Test the model on unseen data (test set) using metrics like accuracy, RMSE, F1-score, etc."
      ],
      "metadata": {
        "id": "L4USSf4Ujo7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q11- Why do we have to perform EDA before fitting a model to the data?**\n",
        "\n",
        "ANS- Exploratory Data Analysis (EDA) is a crucial step before fitting a machine learning model because it helps us understand the dataset, identify patterns, and detect potential issues that could affect model performance. EDA involves visualizing and summarizing the data to uncover relationships, trends, and anomalies, which ensures that the model is trained on clean and meaningful data.\n",
        "\n",
        "**Performing EDA helps in several ways:**\n",
        "\n",
        "**Identify missing values** – Missing data can be handled through imputation or removal.\n",
        "\n",
        "**Detect outliers** – Extreme values can distort model training if not addressed.\n",
        "\n",
        "**Understand variable distributions** – Knowing whether a variable is normally distributed or skewed helps in choosing preprocessing techniques and models.\n",
        "\n",
        "**Discover relationships between variables** – Correlations or dependencies between features can guide feature selection or engineering.\n",
        "\n",
        "**Inform model selection** – EDA can suggest which algorithms may work best based on the nature of the data.\n"
      ],
      "metadata": {
        "id": "73GZwlD0jo4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q12- What is correlation?**\n",
        "\n",
        "ANS- Correlation is a statistical measure that describes the strength and direction of a relationship between two variables. It indicates how one variable changes in response to changes in another variable.\n",
        "\n",
        "A positive correlation means that as one variable increases, the other also increases.\n",
        "\n",
        "A negative correlation means that as one variable increases, the other decreases.\n",
        "\n",
        "A correlation of 0 indicates no linear relationship between the variables.\n",
        "\n",
        "The correlation is usually quantified using the correlation coefficient, which ranges from –1 to +1.\n",
        "\n",
        "**Example:** There is a positive correlation between hours studied and exam scores—as study hours increase, exam scores tend to increase. Conversely, there is a negative correlation between car age and resale value—as the car gets older, its resale value decreases."
      ],
      "metadata": {
        "id": "zCfDMQ0wnRJZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q13- What does negative correlation mean?**\n",
        "\n",
        "ANS- Negative correlation refers to a relationship between two variables in which one variable increases while the other decreases. In other words, the variables move in opposite directions. The correlation coefficient for a negative correlation ranges between –1 and 0, with –1 indicating a perfect inverse relationship.\n",
        "\n",
        "**Example:** Consider the relationship between the price of a product and its demand. Typically, as the price of a product rises, the demand decreases, and when the price falls, the demand increases. This inverse relationship is an example of negative correlation. Another example is the amount of exercise and body weight; generally, as exercise increases, body weight may decrease."
      ],
      "metadata": {
        "id": "MHT8u_M6nRF3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q14- How can you find correlation between variables in Python?**\n",
        "\n",
        "ANS- In Python, you can find the correlation between variables using libraries like Pandas and Scipy. Correlation measures how strongly two variables are related and the direction of their relationship.\n",
        "\n",
        "1. **Using Pandas corr()**\n",
        "\n",
        "The corr() function calculates the correlation matrix for all numerical columns in a DataFrame.\n",
        "\n",
        "2. **Visualizing correlation with Seaborn**\n",
        "\n",
        "Visualizing correlation with Seaborn is a way to graphically represent the relationships between numerical variables in a dataset.\n",
        "\n",
        "3. **Using Scipy for two variables**\n",
        "\n",
        "Using Scipy for two variables is a way to calculate the correlation coefficient specifically between two numerical variables and also assess its statistical significance."
      ],
      "metadata": {
        "id": "38q1hnYWnRDw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q15- What is causation? Explain difference between correlation and causation with an example.**\n",
        "\n",
        "ANS- **Causation** refers to a relationship where one variable directly affects or causes a change in another variable. In other words, a causal relationship implies that changes in the independent variable lead to changes in the dependent variable.\n",
        "\n",
        "**Difference between Correlation and Causation:**\n",
        "\n",
        "1. Correlation measures the strength and direction of a relationship; Causation shows a direct cause-effect relationship.\n",
        "\n",
        "2. Correlation does not imply that one variable causes the other; causation does.\n",
        "\n",
        "3. Correlation is quantified by a coefficient (–1 to +1); causation is not measured by a coefficient.\n",
        "\n",
        "4. **Example of correlation:** Ice cream sales and drowning incidents rise together in summer—they are correlated but one doesn’t cause the other.\n",
        "\n",
        "5. **Example of causation:** Smoking causes lung cancer—smoking directly leads to increased risk."
      ],
      "metadata": {
        "id": "gcrsswUODdyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q16- What is an Optimizer? What are different types of optimizers? Explain each with an example.**\n",
        "\n",
        "ANS- Optimizer in machine learning is an algorithm used to update the model’s parameters (like weights and biases) during training to minimize the loss function. Essentially, it helps the model learn by improving its predictions and reducing errors.\n",
        "\n",
        "**Different Types of Optimizers**\n",
        "\n",
        "1. **Gradient Descent (GD)**\n",
        "\n",
        "   Updates parameters using the gradient of the loss function computed on the entire dataset.\n",
        "\n",
        "   Pros: Simple and effective for small datasets.\n",
        "\n",
        "   Cons: Can be slow for large datasets.\n",
        "\n",
        "   Example: Linear regression model trained with all data points in each step.\n",
        "\n",
        "2. **Stochastic Gradient Descent (SGD)**\n",
        "\n",
        "   Updates parameters using the gradient calculated from one training sample at a time.\n",
        "\n",
        "   Pros: Faster updates and can escape local minima.\n",
        "\n",
        "   Cons: Updates are noisy, may oscillate around the minimum.\n",
        "\n",
        "   Example: Training a neural network where weights are updated after each sample.\n",
        "\n",
        "3. **Mini-batch Gradient Descent**\n",
        "\n",
        "   Uses a small batch of samples to compute the gradient in each step.\n",
        "\n",
        "   Pros: Balances efficiency and stability.\n",
        "\n",
        "   Example: Training a deep learning model with batch size 32.\n",
        "\n",
        "4. **Momentum**\n",
        "\n",
        "   Accelerates GD by considering past gradients to smooth updates and avoid oscillations.\n",
        "\n",
        "   Example: Updating weights in deep networks to converge faster.\n",
        "\n"
      ],
      "metadata": {
        "id": "dG-khEQFDdu6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q17- What is sklearn.linear_model ?**\n",
        "\n",
        "ANS- sklearn.linear_model is a module in the scikit-learn library that provides a variety of linear models for regression and classification tasks. These models assume a linear relationship between input features (independent variables) and the output (dependent variable).\n",
        "\n",
        "**Key Features:**\n",
        "\n",
        "Implements both simple and regularized linear models.\n",
        "\n",
        "Supports regression (predicting continuous values) and classification (predicting categorical labels).\n",
        "\n",
        "Easy to integrate with scikit-learn workflows like preprocessing, model fitting, and evaluation"
      ],
      "metadata": {
        "id": "d9YtlSD8DdsT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q18- What does model.fit() do? What arguments must be given?**\n",
        "\n",
        "ANS- model.fit() is a method used to train a machine learning model. When called, it allows the model to learn patterns from the training data by adjusting its internal parameters (like weights and biases) to minimize the error or loss function. Essentially, this is how the model “fits” itself to the data.\n",
        "\n",
        "**Arguments Required:**\n",
        "\n",
        "X (features/input data): A 2D array or DataFrame containing the independent variables.\n",
        "\n",
        "y (target/output data): A 1D or 2D array containing the dependent variable(s) that the model is trying to predict."
      ],
      "metadata": {
        "id": "7MKzkONKDdpZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q19- What does model.predict() do? What arguments must be given?**\n",
        "\n",
        "ANS- model.predict() is a method in machine learning used to make predictions using a trained model. Once a model has been trained with model.fit(), predict() takes new input data and outputs the model’s predicted values based on the patterns it has learned.\n",
        "\n",
        "**Arguments Required:**\n",
        "\n",
        "X (features/input data): A 2D array or DataFrame containing the independent variables for which you want predictions.\n",
        "\n",
        "**Syntax:**\n",
        "\n",
        "predictions = model.predict(X)"
      ],
      "metadata": {
        "id": "1931gmweDdmw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q20- What are continuous and categorical variables?**\n",
        "\n",
        "ANS- **1. Continuous Variables**\n",
        "\n",
        "Numerical variables that can take any value within a range.\n",
        "\n",
        "Measurable and often have decimal points.\n",
        "\n",
        "Represent quantities that can vary smoothly.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Height (e.g., 5.7 ft)\n",
        "\n",
        "Weight (e.g., 60.5 kg)\n",
        "\n",
        "Temperature (e.g., 25.3°C)\n",
        "\n",
        "Age (e.g., 23.5 years)\n",
        "\n",
        "2. **Categorical Variables**\n",
        "\n",
        "Variables that represent distinct categories or groups.\n",
        "\n",
        "Usually qualitative rather than numerical.\n",
        "\n",
        "Can be nominal (no order) or ordinal (ordered categories).\n",
        "\n",
        "Examples:\n",
        "\n",
        "Gender: Male, Female\n",
        "\n",
        "Color: Red, Blue, Green\n",
        "\n",
        "Education Level: High School, Bachelor’s, Master’s (ordinal)"
      ],
      "metadata": {
        "id": "2dHT6cimGb26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q21- What is feature scaling? How does it help in Machine Learning?**\n",
        "\n",
        "ANS- Feature scaling is a technique used in machine learning to standardize or normalize the range of independent variables (features) so that they are on a similar scale. This ensures that no single feature dominates the model just because it has larger numerical values.\n",
        "\n",
        "**Why Feature Scaling is Important**\n",
        "\n",
        "**Prevents dominance of large-value features:**\n",
        "Algorithms like gradient descent, K-Nearest Neighbors (KNN), SVM, and neural networks are sensitive to the magnitude of features. Without scaling, features with larger values can disproportionately influence the model.\n",
        "\n",
        "**Speeds up convergence:**\n",
        "Gradient-based optimization algorithms converge faster when features are on a similar scale.\n",
        "\n",
        "**Improves model performance:**\n",
        "Scaled data often results in better accuracy and stability\n",
        "\n"
      ],
      "metadata": {
        "id": "chWvUqToGcpe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q22- How do we perform scaling in Python?**\n",
        "\n",
        "ANS- In Python, feature scaling is commonly performed using scikit-learn’s preprocessing module. The two most popular techniques are Standardization and Normalization (Min-Max Scaling).\n",
        "\n",
        "1. **Standardization (Z-score Scaling)**\n",
        "\n",
        "   Scales the features so that they have mean = 0 and standard deviation = 1. Useful for algorithms like linear regression, logistic regression, and SVM.\n",
        "\n",
        "2. **Min-Max Scaling (Normalization)**\n",
        "\n",
        "   Scales the features to a fixed range, usually [0, 1]. Useful for algorithms like KNN or neural networks.\n",
        "\n",
        "**Steps for Scaling in Python**\n",
        "\n",
        "Import the scaler from sklearn.preprocessing.\n",
        "\n",
        "Create an instance of the scaler.\n",
        "\n",
        "Fit the scaler to your data using .fit() (calculates mean/min/max).\n",
        "\n",
        "Transform the data using .transform() or directly use .fit_transform()"
      ],
      "metadata": {
        "id": "JE45e-Z9Gcl9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q23- What is sklearn.preprocessing?**\n",
        "\n",
        "ANS- sklearn.preprocessing is a module in the scikit-learn library that provides tools for preprocessing and transforming data before feeding it into a machine learning model. Since most ML algorithms require numerical inputs and benefit from standardized or normalized data, this module helps prepare data to improve model performance and training efficiency.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "TzYFn9xGGcjT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q24- How do we split data for model fitting (training and testing) in Python?**\n",
        "\n",
        "ANS- In Python, we typically split data into training and testing sets using the train_test_split function from scikit-learn. This allows the model to learn patterns from the training data and evaluate its performance on unseen testing data.\n",
        "\n",
        "**Parameters:**\n",
        "\n",
        "X → input features (independent variables)\n",
        "\n",
        "y → target variable (dependent variable)\n",
        "\n",
        "test_size → proportion of the dataset to include in the test split (e.g., 0.2 means 20% for testing)\n",
        "\n",
        "random_state → seed for reproducibility"
      ],
      "metadata": {
        "id": "gmaYxrOQGcga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q25- Explain data encoding?**\n",
        "\n",
        "ANS-Data encoding is the process of converting categorical data into numerical format so that machine learning algorithms can process it. Most ML algorithms cannot work directly with text or labels; they require numbers to perform calculations, measure distances, or optimize loss functions.\n",
        "\n",
        "**Types of Data Encoding**\n",
        "\n",
        "**Label Encoding**\n",
        "\n",
        "Assigns a unique integer to each category.\n",
        "\n",
        "Suitable for ordinal categories (where order matters).\n",
        "\n",
        "**One-Hot Encoding**\n",
        "\n",
        "Creates binary columns for each category.\n",
        "\n",
        "Suitable for nominal categories (no natural order)."
      ],
      "metadata": {
        "id": "2ecnybSnGcc-"
      }
    }
  ]
}